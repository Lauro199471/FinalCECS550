{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CECS 551 Assignment 4 Team 4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmmd1_6J72bO",
        "colab_type": "text"
      },
      "source": [
        "CECS 551 Assignment 4  \n",
        "Team 4: Sella Bae, Matthew Nguyen, Yashua Ovando  \n",
        "Feb 21, 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wti3eygJVOuJ",
        "colab_type": "text"
      },
      "source": [
        "# Multi-layer neural network model from scratch\n",
        "Train a model that approximates XOR function\n",
        "- Layer1 width: 2\n",
        "- Layer1 activation func: [hyperbolic tangent](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh)\n",
        "- Layer2 width: 1\n",
        "- Layer2 activation func: [sigmoid](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid)\n",
        "- Loss function: [binary cross entropy](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) $L(y,\\hat{y}) = âˆ’(ð‘¦log(\\hat{y})+(1âˆ’ð‘¦)log(1âˆ’\\hat{y}))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhZfOzzCW9Rf",
        "colab_type": "text"
      },
      "source": [
        "### (b) Calculation of $\\frac{\\partial L}{\\partial \\vec{W}^{(1)}}$, $\\frac{\\partial L}{\\partial \\vec{w}^{(2)}}$, $\\frac{\\partial L}{\\partial \\vec{b}^{(1)}}$, and $\\frac{\\partial L}{\\partial {b}^{(2)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEAp-yxBcDpH",
        "colab_type": "text"
      },
      "source": [
        "$\n",
        "\\frac{\\partial L}{\\partial \\vec{W}^{(1)}}\n",
        "= \\left[ \\begin{matrix}\n",
        "\\frac{\\partial L}{\\partial w^1_{11}} &\n",
        "\\frac{\\partial L}{\\partial w^1_{12}} \\\\\n",
        "\\frac{\\partial L}{\\partial w^1_{21}} &\n",
        "\\frac{\\partial L}{\\partial w^1_{22}}\n",
        "\\end{matrix} \\right]\n",
        "= J'(y,\\hat{y}) \\cdot \\sigma'(\\hat{y}) \\cdot {\\vec{w}^{(2)}}^\\top \\cdot {\\tanh'(\\vec{h}^{(1)})}^\\top \\cdot \\vec{x}\n",
        "$\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial w^1_{11}}\n",
        "= \\frac{\\partial L}{\\partial h^1_1}\n",
        "\\frac{\\partial h^1_1}{\\partial a^1_1}\n",
        "\\frac{\\partial a^1_1}{\\partial w^1_{11}}\n",
        "= (\\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat y})\n",
        "\\cdot \\hat{y}(1-\\hat{y}) \n",
        "\\cdot w^2_1\n",
        "\\cdot (1-(h^1_1)^2)  \n",
        "\\cdot x_1\n",
        "$\n",
        "> $\\frac{\\partial L}{\\partial w^1_{12}}\n",
        "= \\frac{\\partial L}{\\partial h^1_2}\n",
        "\\frac{\\partial h^1_2}{\\partial a^1_2}\n",
        "\\frac{\\partial a^1_2}{\\partial w^1_{12}}\n",
        "= (\\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat y})\n",
        " \\cdot \\hat{y}(1-\\hat{y})\n",
        " \\cdot w^2_2 \n",
        " \\cdot (1-(h^1_2)^2)  \n",
        " \\cdot x_1\n",
        "$\n",
        "> $\\frac{\\partial L}{\\partial w^1_{21}}\n",
        "= \\frac{\\partial L}{\\partial h^1_1}\n",
        "\\frac{\\partial h^1_1}{\\partial a^1_1}\n",
        "\\frac{\\partial a^1_1}{\\partial w^1_{21}}\n",
        "= (\\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat y})\n",
        "\\cdot \\hat{y}(1-\\hat{y}) \n",
        "\\cdot w^2_1 \n",
        "\\cdot (1-(h^1_1)^2)  \n",
        "\\cdot x_2\n",
        "$\n",
        "> $\\frac{\\partial L}{\\partial w^1_{22}}\n",
        "= \\frac{\\partial L}{\\partial h^1_2}\n",
        "\\frac{\\partial h^1_2}{\\partial a^1_2}\n",
        "\\frac{\\partial a^1_2}{\\partial w^1_{22}}\n",
        "= (\\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat y}) \n",
        "\\cdot \\hat{y}(1-\\hat{y}) \n",
        "\\cdot w^2_2 \n",
        "\\cdot (1-(h^1_2)^2)  \n",
        "\\cdot x_2\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYjwjctmjljF",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{\\partial L}{\\partial \\vec{w}^{(2)}}\n",
        "= \\left[ \\begin{matrix}\n",
        "\\frac{\\partial L}{\\partial w^2_1} \\\\\n",
        "\\frac{\\partial L}{\\partial w^2_2}\n",
        "\\end{matrix} \\right]\n",
        "= J'(y,\\hat{y})\n",
        "\\cdot \\sigma'(\\hat{y})\n",
        "\\cdot \\vec{h}^{(1)}\n",
        "$\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial w^2_1}\n",
        "= \\frac{\\partial L}{\\partial \\hat{y}}\n",
        "\\frac{\\partial h^2_1}{\\partial a^2_1}\n",
        "\\frac{\\partial a^2_1}{\\partial w^2_1}\n",
        "= (\\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat y})\n",
        "\\cdot \\hat{y}(1-\\hat{y})\n",
        "\\cdot h^1_1\n",
        "$  \n",
        "> $\\frac{\\partial L}{\\partial w^2_2}\n",
        "= \\frac{\\partial L}{\\partial \\hat{y}}\n",
        "\\frac{\\partial h^2_1}{\\partial a^2_1}\n",
        "\\frac{\\partial a^2_1}{\\partial w^2_2}\n",
        "= (\\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat y})\n",
        "\\cdot \\hat{y}(1-\\hat{y})\n",
        "\\cdot h^1_2\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77O2pi8ZjnDx",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{\\partial L}{\\partial \\vec b^{(1)}}\n",
        "= \\left[ \\begin{matrix}\n",
        "\\frac{\\partial L}{\\partial b^1_1} \\\\\n",
        "\\frac{\\partial L}{\\partial b^1_2}\n",
        "\\end{matrix} \\right]\n",
        "= J'(y,\\hat{y}) \\cdot \\sigma'(\\hat y) \\cdot {\\vec w^{(2)}}^\\top \\cdot {tanh'(\\vec h^{(1)})}^\\top\n",
        "$\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial b^1_1} =\n",
        "\\frac{\\partial L}{\\partial h^1_1}\n",
        "\\frac{\\partial h^1_1}{\\partial a^1_1}\n",
        "\\frac{\\partial a^1_1}{\\partial b^1_1}\n",
        "= (\\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat y})\n",
        "\\cdot \\hat{y}(1-\\hat{y}) \\cdot w^2_1 \\cdot (1-(h^1_1)^2) \\cdot 1\n",
        "$  \n",
        "> $\\frac{\\partial L}{\\partial b^1_2} =\n",
        "\\frac{\\partial L}{\\partial h^1_2}\n",
        "\\frac{\\partial h^1_2}{\\partial a^1_2}\n",
        "\\frac{\\partial a^1_2}{\\partial b^1_2}\n",
        "= (\\frac{1-y}{1-\\hat{y}} - \\frac{y}{\\hat y})\n",
        "\\cdot \\hat{y}(1-\\hat{y}) \\cdot w^2_2 \\cdot (1-(h^1_2)^2) \\cdot 1\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzSv3EHSjo-I",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{\\partial L}{\\partial b^{(2)}}\n",
        "= \\frac{\\partial L}{\\partial b^2_1}\n",
        "= J'(y,\\hat{y}) \\cdot \\sigma'(\\hat{y})\n",
        "$\n",
        "\n",
        "> $\\frac{\\partial L}{\\partial b^2_1}\n",
        "= \\frac{\\partial L}{\\partial \\hat y}\n",
        "\\frac{\\partial h^2_1}{\\partial a^2_1}\n",
        "\\frac{\\partial a^2_1}{\\partial b^2_1}\n",
        "= (\\frac{1-y}{1 - \\hat y} - \\frac{y}{\\hat y}) \\cdot \\hat{y}(1-\\hat{y}) \\cdot 1\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShQSmIV_fpu0",
        "colab_type": "text"
      },
      "source": [
        "### (c) Implementation of the XOR model\n",
        "without using external deep learning libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IECvxxPmx5s",
        "colab_type": "text"
      },
      "source": [
        "#### Sets inputs and targets of XOR function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIhThzqCfcyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])   #train data\n",
        "y = np.array([0,1,1,0])                   #target\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwindTd87SqS",
        "colab_type": "text"
      },
      "source": [
        "#### Defines the activation functions, loss function, and their derivatives to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bct2D2mM7YET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1.0 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "  return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "def tanh(z):\n",
        "\treturn (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
        "  # return np.tanh(z)\n",
        "\n",
        "def tanh_derivative(z):\n",
        "  return 1 - np.power(tanh(z), 2)\n",
        "\n",
        "def cross_entropy(y, yHat):\n",
        "  if y == 1:\n",
        "     return -np.log(yHat)\n",
        "  else:\n",
        "     return -np.log(1 - yHat)\n",
        "\n",
        "def cross_entropy_derivative(y, yHat):\n",
        "   if y == 1:\n",
        "     return -(1/yHat)\n",
        "   else:\n",
        "     return 1/(1-yHat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jzRJocCmQO7",
        "colab_type": "text"
      },
      "source": [
        "#### Initializes weights and biases with random numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ayyUMotfnmX",
        "colab_type": "code",
        "outputId": "cac6fab1-cf4c-4a95-90a9-7401c1be048f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# layer1 (hidden layer)\n",
        "W1 = np.random.uniform(size=(2,2))\n",
        "B1 = np.random.uniform(size=(1,2))\n",
        "# layer2 (output layer)\n",
        "W2 = np.random.uniform(size=(2,1))\n",
        "B2 = np.random.uniform(size=(1,1))\n",
        "\n",
        "def print_params():\n",
        "  print('W1', W1) #hidden layer weights\n",
        "  print('B1', B1) #hidden layer bias\n",
        "  print('W2', W2) #output layer weights\n",
        "  print('B2', B2) #output layer bias\n",
        "\n",
        "print('initial params')\n",
        "print_params()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial params\n",
            "W1 [[0.5493904  0.34114756]\n",
            " [0.66874367 0.4247187 ]]\n",
            "B1 [[0.81988232 0.20774709]]\n",
            "W2 [[0.48298607]\n",
            " [0.66560286]]\n",
            "B2 [[0.76986075]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6GJ-mE8-Sef",
        "colab_type": "text"
      },
      "source": [
        "#### Trains the model!\n",
        "- Update after each sample (batch size 1)\n",
        "- learning rate: 0.1\n",
        "- epoch: 1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhKkUvGT-aWM",
        "colab_type": "code",
        "outputId": "09543aee-a792-40bd-ff14-485fc78024dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "lr = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "predicted = np.zeros(4, dtype=float)    # store prediction of 4 samples\n",
        "losses = np.zeros(epochs, dtype=float)   # store loss from each epoch\n",
        "\n",
        "\n",
        "for k in range(epochs):\n",
        "\n",
        "  # For all samples\n",
        "  for i in range(len(X)):\n",
        "\n",
        "    # Forward propagation\n",
        "    A1 = np.dot(X[i], W1) + B1      # hidden layer apply params 1x2\n",
        "    H1 = tanh(A1)                   # hidden layer activation function 1x2\n",
        "    A2 = np.dot(H1, W2) + B2        # output layer apply params 1x1\n",
        "    H2 = sigmoid(A2)                # output layer activation function 1x1\n",
        "    predicted[i] = H2               # yHat\n",
        "    \n",
        "    # Calculate Loss\n",
        "    loss = cross_entropy(y[i], predicted[i])\n",
        "\n",
        "    # Backward Propogation\n",
        "    # Update Weight2\n",
        "    d_yHat = cross_entropy_derivative(y[i], predicted[i]);\n",
        "    d_A2 = sigmoid_derivative(A2)       #1x1\n",
        "    daa2 = d_yHat * d_A2                #1x1\n",
        "    d_W2 = H1                           #1x2\n",
        "    W2_update = np.dot(daa2, d_W2).T    #2x1\n",
        "    W2 -= lr * W2_update\n",
        "    # Update Bias2\n",
        "    B2_update = daa2                    #1x1\n",
        "    B2 -= lr * B2_update\n",
        "    # Update Weight1\n",
        "    d_H1 = d_A2 * d_yHat * W2.T         #1x2\n",
        "    d_A1 = tanh_derivative(A1)          #1x2\n",
        "    d_W1 = np.matrix(X[i]).T            #2x1\n",
        "    daa1 = np.multiply(d_H1, d_A1)      #1x2\n",
        "    W1_update = np.multiply(d_W1, daa1) #2x2\n",
        "    W1 -= lr * W1_update\n",
        "    # Update Bias1\n",
        "    B1_update = daa1                    #1x2\n",
        "    B1 -= lr * B1_update\n",
        "\n",
        "  # print(k, 'predicted', predicted, 'loss', loss)\n",
        "  losses[k] = loss\n",
        "\n",
        "\n",
        "print('after', epochs, 'epochs')\n",
        "print('target:', y)\n",
        "print('predicted:', predicted)\n",
        "print('loss:', f'{loss:.{8}f}')\n",
        "\n",
        "print('\\nfinal params');\n",
        "print_params()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after 1000 epochs\n",
            "target: [0 1 1 0]\n",
            "predicted: [0.01605761 0.99085424 0.99083447 0.01776311]\n",
            "loss: 0.01792277\n",
            "\n",
            "final params\n",
            "W1 [[2.54847504 3.7022514 ]\n",
            " [2.54653541 3.71245332]]\n",
            "B1 [[-3.71997784 -1.74936696]]\n",
            "W2 [[-5.22594097]\n",
            " [ 5.10466194]]\n",
            "B2 [[-4.53176221]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id5bC3Lh3tgr",
        "colab_type": "text"
      },
      "source": [
        "#### Checks the change in loss through each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHC_cIRS1s0A",
        "colab_type": "code",
        "outputId": "d25020e2-2708-4d51-fbd8-9d0d2dc76b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.arange(epochs), losses)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.ylim(0, 2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns4TJp90AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}